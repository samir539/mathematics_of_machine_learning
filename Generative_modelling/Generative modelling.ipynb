{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23217184",
   "metadata": {},
   "source": [
    "We look to:\n",
    "\n",
    "- Build a variational auto-encoder\n",
    "- Use it to generate images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9227ab",
   "metadata": {},
   "source": [
    "# Building a variational auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f20b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a000c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim, hidden_dims=None):\n",
    "        super().__init__() # we need this so it inherits everything from the class nn\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Build encoder; it is standard for the encoder's number of channels to increase\n",
    "        modules = []\n",
    "        if hidden_dims is None: # define a certain number  configuration for the encoder\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        for h_dim in hidden_dims: \n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim, kernel_size = 3, stride = 2, padding = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim \n",
    "        \n",
    "        # the encoder consist of the list modules which is len(hidden_dims) times the sequence \n",
    "        # we defined above in nn.Sequential\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        # now we will define the mu and sigma layers (remember: encoder maps our x into mu and sigma (=\\psi)\n",
    "        # which will be the input for our q_{\\psi}(z|x) to sample z)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim) \n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        \n",
    "        # Build decoder (takes as input z of dimensionality latent_dim)\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n",
    "        hidden_dims.reverse() # same structure as hidden_dims but other way around\n",
    "        \n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i], # this is a deconvolution layer\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        # final layer then computes tanh(something); this is the mu and we assume sigma=Identity\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= 3,\n",
    "                                      kernel_size= 3, padding= 1),\n",
    "                            nn.Tanh())\n",
    "    \n",
    "    def encode(self, input):\n",
    "        \"\"\" \n",
    "        Encodes the input by passing it through the encoder network and returning z (the latent variable) in list form\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1) # only dimensions starting with start_dim are flattened\n",
    "        # Split the result into mu and var; each of dimension latent_dim\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        return [mu, log_var]\n",
    "        \n",
    "    def decode(self, z): # this is in essense a single sample from p_{\\theta}(x|z)=Gaussian(output,I)\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 512, 2, 2) # we need this to change z to the right dimensions for conv layers\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "    \n",
    "    def reparametrize(self, mu, var):\n",
    "        \"\"\"\n",
    "        does the reparametrizatrion trick\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "    \n",
    "    def forward(self, input):\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparametrize(mu, log_var) # sample a random z\n",
    "        return [self.decode(z), input, mu, log_var]\n",
    "    \n",
    "    def loss_function(self, *args):\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "        return recons_loss + 0.0001*kld_loss\n",
    "        \n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        Samples from the latent space and returns the corresponding input (basically just passing through decoder)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33d6bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data_faces/celeba', <http.client.HTTPMessage at 0x28163ba7880>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "fullfilename = os.path.join('./data_faces/', 'celeba')\n",
    "urlretrieve(\"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \", fullfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43fce1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202599\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"./data_faces/celeba\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data_faces/\")\n",
    "\n",
    "root = 'data_faces/img_align_celeba'\n",
    "img_list = os.listdir(root)\n",
    "print(len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc53f671",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_SingleProcessDataLoaderIter' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m data_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(celeba_data,batch_size\u001b[39m=\u001b[39mbatch_size,shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(data_loader)\n\u001b[1;32m---> 21\u001b[0m img, labels \u001b[39m=\u001b[39m dataiter\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m     22\u001b[0m plt\u001b[39m.\u001b[39mimshow(np\u001b[39m.\u001b[39mtranspose(img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy(), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)))\n\u001b[0;32m     23\u001b[0m plt\u001b[39m.\u001b[39mplot()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_SingleProcessDataLoaderIter' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# Crop and scale the data\n",
    "crop_size = 108\n",
    "re_size = 64\n",
    "offset_height = (218 - crop_size) // 2\n",
    "offset_width = (178 - crop_size) // 2\n",
    "crop = lambda x: x[:, offset_height:offset_height + crop_size, offset_width:offset_width + crop_size]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Lambda(crop),\n",
    "     transforms.ToPILImage(),\n",
    "     transforms.Resize(size=(re_size, re_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)])\n",
    "\n",
    "batch_size = 64\n",
    "celeba_data = torchvision.datasets.ImageFolder('./data_faces/', transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(celeba_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "dataiter = iter(data_loader)\n",
    "img, labels = dataiter.next()\n",
    "plt.imshow(np.transpose(img[0].numpy(), (1, 2, 0)))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33bf85cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.2190493941307068\n",
      "20 0.1481684148311615\n",
      "30 0.11184293776750565\n",
      "40 0.11844523996114731\n",
      "50 0.09888102114200592\n",
      "60 0.08754809945821762\n",
      "70 0.08022349327802658\n",
      "80 0.08917193114757538\n",
      "90 0.07941785454750061\n",
      "100 0.08565007895231247\n",
      "110 0.07814276963472366\n",
      "120 0.0798088014125824\n",
      "130 0.07963082194328308\n",
      "140 0.07748065143823624\n",
      "150 0.07393917441368103\n",
      "160 0.06918950378894806\n",
      "170 0.07207281142473221\n",
      "180 0.067186638712883\n",
      "190 0.06815145164728165\n",
      "200 0.06659197062253952\n",
      "210 0.061425477266311646\n",
      "220 0.06252864003181458\n",
      "230 0.061073508113622665\n",
      "240 0.06800016015768051\n",
      "250 0.06448479741811752\n",
      "260 0.06115265190601349\n",
      "270 0.06915315985679626\n",
      "280 0.061398863792419434\n",
      "290 0.05872933566570282\n",
      "300 0.06793393939733505\n",
      "310 0.062022771686315536\n",
      "320 0.060304343700408936\n",
      "330 0.05263516679406166\n",
      "340 0.056093860417604446\n",
      "350 0.05368071049451828\n",
      "360 0.06266869604587555\n",
      "370 0.060806095600128174\n",
      "380 0.054120007902383804\n",
      "390 0.05770733952522278\n",
      "400 0.06139296293258667\n",
      "410 0.05731608346104622\n",
      "420 0.05732051655650139\n",
      "430 0.053251154720783234\n",
      "440 0.050676196813583374\n",
      "450 0.05664600431919098\n",
      "460 0.047703273594379425\n",
      "470 0.05508524179458618\n",
      "480 0.05185079947113991\n",
      "490 0.05783404782414436\n",
      "500 0.055625271052122116\n",
      "510 0.04843132197856903\n",
      "520 0.04780418053269386\n",
      "530 0.05509302765130997\n",
      "540 0.04644659534096718\n",
      "550 0.04598679393529892\n",
      "560 0.0528012216091156\n",
      "570 0.04954000562429428\n",
      "580 0.04406377300620079\n",
      "590 0.056387387216091156\n",
      "600 0.04705179110169411\n",
      "610 0.04620540142059326\n",
      "620 0.04572984203696251\n",
      "630 0.048110101372003555\n",
      "640 0.048293836414813995\n",
      "650 0.04443127661943436\n",
      "660 0.04627698287367821\n",
      "670 0.04725594073534012\n",
      "680 0.046962663531303406\n",
      "690 0.04709650203585625\n",
      "700 0.048914819955825806\n",
      "710 0.04940709471702576\n",
      "720 0.046061888337135315\n",
      "730 0.04750872030854225\n",
      "740 0.04449985548853874\n",
      "750 0.04569125920534134\n",
      "760 0.045065537095069885\n",
      "770 0.049582354724407196\n",
      "780 0.047019731253385544\n",
      "790 0.046474870294332504\n",
      "800 0.044413235038518906\n",
      "810 0.04704112559556961\n",
      "820 0.045636601746082306\n",
      "830 0.044307343661785126\n",
      "840 0.04510257765650749\n",
      "850 0.05056314915418625\n",
      "860 0.04562538489699364\n",
      "870 0.049198515713214874\n",
      "880 0.048839494585990906\n",
      "890 0.04581672325730324\n",
      "900 0.043050479143857956\n",
      "910 0.04382794722914696\n",
      "920 0.04192724823951721\n",
      "930 0.04347264766693115\n",
      "940 0.045815359801054\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "epochs = 1\n",
    "in_channels = 3\n",
    "latent_dim = 100\n",
    "vae = VAE(in_channels=in_channels, latent_dim=latent_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "step = 0\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for idx, (img_batch, _) in enumerate(data_loader):\n",
    "        step += 1\n",
    "        mbatch_size = img_batch.size()[0]\n",
    "        # forward pass \n",
    "        [a,b,c,d] = vae.forward(img_batch)\n",
    "        loss = vae.loss_function(a,b,c,d)\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "        # Take the optimisation step\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        losses += [loss.item()]\n",
    "        if step % 10 == 0:    # print every 100 iterations\n",
    "            print(step, loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample \n",
    "num_samples = 10\n",
    "outputs = vae.sample(num_samples)\n",
    "numpy_outputs = outputs.detach().numpy()\n",
    "print(numpy_outputs.shape)\n",
    "for it in range(num_samples):\n",
    "    plt.imshow(np.transpose(numpy_outputs[it], (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7effc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
